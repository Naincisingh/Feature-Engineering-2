{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d82be0f0-25a4-474e-a8d4-dbc3e2732504",
   "metadata": {},
   "source": [
    "QTS.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d81918-6bf4-4538-80c3-8898ff6aaa70",
   "metadata": {},
   "source": [
    "**Filter Method in Feature Selection:**\n",
    "- **Definition**: Filter methods evaluate the relevance of features based \n",
    "on statistical measures and rank or select them before the model training process.\n",
    "- **How it works**: Features are assessed independently of the machine learning model.\n",
    "Common techniques include correlation, chi-squared tests, information gain, and \n",
    "mutual information. Features are then ranked or selected based on these measures,\n",
    "and only the top-ranked features are used for model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65b9357-61d0-4207-9a06-c93e0d2877ab",
   "metadata": {},
   "source": [
    "QTS.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11bbcf5-04c1-4982-bd9c-f49c1fa7e98f",
   "metadata": {},
   "source": [
    "**Wrapper Method vs. Filter Method:**\n",
    "- **Wrapper Method**: Evaluates subsets of features by using a specific \n",
    "machine learning model's performance as a criterion. \n",
    "It involves iterative model training with different feature subsets, and\n",
    "the selection criterion is based on the model's performance (e.g., accuracy, F1 score).\n",
    "- **Filter Method**: Assesses the relevance of features independently of the machine learning \n",
    "model. It relies on statistical measures to rank or select features before the model training process.\n",
    "\n",
    "In summary, the main difference lies in how they assess feature relevance: Wrapper methods\n",
    "use the model's performance, while filter methods use statistical measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6479e5-ad7a-4a67-9e5a-a1acdcfa9e68",
   "metadata": {},
   "source": [
    "QTS.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ea33c4-c5d2-4d81-b520-fbe6bc0bc7cc",
   "metadata": {},
   "source": [
    "**Common Techniques in Embedded Feature Selection:**\n",
    "1. **Lasso Regression (L1 Regularization):**\n",
    "   - **How it works**: Penalizes the absolute \n",
    "    values of the coefficients, encouraging sparsity and automatic feature selection.\n",
    "  \n",
    "2. **Decision Trees and Random Forests:**\n",
    "   - **How it works**: Feature importance scores are calculated during the \n",
    "    tree-building process, aiding in feature selection.\n",
    "\n",
    "3. **Gradient Boosting Machines:**\n",
    "   - **How it works**: Iterative model training where each new model corrects \n",
    "    errors of the previous ones; feature importance is derived during this process.\n",
    "\n",
    "4. **Elastic Net Regression:**\n",
    "   - **How it works**: Combines L1 and L2 regularization, providing a balance \n",
    "    between sparsity and grouping effects for feature selection.\n",
    "\n",
    "5. **Recursive Feature Elimination (RFE):**\n",
    "   - **How it works**: Iteratively removes the least important features based on \n",
    "    model performance until the desired number of features is reached.\n",
    "\n",
    "Embedded methods incorporate feature selection within the model training process,\n",
    "optimizing both the model and feature subset simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f235c5-541e-4b44-afda-ab2a01739484",
   "metadata": {},
   "source": [
    "QTS.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb31de83-69f3-4d50-8bb0-451759fde51f",
   "metadata": {},
   "source": [
    "**Drawbacks of Filter Method for Feature Selection:**\n",
    "1. **Independence Assumption:**\n",
    "   - **Issue**: Filter methods assess features independently, ignoring potential \n",
    "    interactions or dependencies between features.\n",
    "   \n",
    "2. **Ignores Model's Performance:**\n",
    "   - **Issue**: The selected features might not be the most relevant for the \n",
    "    specific machine learning model being used, as filter methods don't consider the model's performance.\n",
    "\n",
    "3. **Doesn't Consider Feature Combinations:**\n",
    "   - **Issue**: Filter methods don't evaluate the impact of feature combinations,\n",
    "    potentially missing synergies that could contribute to better model performance.\n",
    "\n",
    "4. **Sensitivity to Feature Scaling:**\n",
    "   - **Issue**: Results can be sensitive to the scale of features, and inappropriate \n",
    "    scaling might impact the ranking of features.\n",
    "\n",
    "5. **Limited in Handling Redundancy:**\n",
    "   - **Issue**: Filter methods may not effectively handle redundant features, \n",
    "    leading to the selection of correlated features.\n",
    "\n",
    "While filter methods are computationally efficient, these drawbacks highlight \n",
    "limitations in their ability to capture the full complexity of relationships between features in a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d30c561-feb4-4073-9bee-7886fcb1e91c",
   "metadata": {},
   "source": [
    "QTS.5\n",
    "\n",
    "**Use Filter Method Over Wrapper Method When:**\n",
    "1. **Large Dataset:**\n",
    "   - **Situation**: Working with a large dataset where the computational cost of\n",
    "    wrapper methods is prohibitive.\n",
    "\n",
    "2. **Computational Efficiency is Critical:**\n",
    "   - **Situation**: Need a quick and computationally efficient feature selection \n",
    "    process without the overhead of iterative model training.\n",
    "\n",
    "3. **Exploratory Data Analysis:**\n",
    "   - **Situation**: Conducting initial data exploration and want a fast way to \n",
    "    identify potentially relevant features before diving into complex model-specific evaluations.\n",
    "\n",
    "4. **Feature Independence is Reasonable:**\n",
    "   - **Situation**: Assumption that features are reasonably independent, and \n",
    "    assessing them individually is sufficient for the task.\n",
    "\n",
    "Filter methods are advantageous in scenarios where quick, independent feature \n",
    "assessment is needed, and computational efficiency is a priority."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897eb91c-5d0c-4629-b2b3-6f92abea00e9",
   "metadata": {},
   "source": [
    "QTS.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fa52d5-f9cc-48b2-bfb5-f9722dbefe68",
   "metadata": {},
   "source": [
    "**Filter Method for Selecting Features in Telecom Customer Churn Model:**\n",
    "1. **Explore Data:**\n",
    "   - **Step**: Conduct initial exploratory data analysis to understand the \n",
    "    characteristics of the dataset and the relationships between features.\n",
    "\n",
    "2. **Calculate Relevance Metrics:**\n",
    "   - **Step**: Use filter methods such as correlation analysis, chi-squared tests,\n",
    "    or information gain to calculate relevance metrics for each feature in isolation.\n",
    "\n",
    "3. **Rank Features:**\n",
    "   - **Step**: Rank the features based on their relevance metrics. \n",
    "    Identify the top-ranked features that show the highest correlation or information\n",
    "    gain concerning the target variable (churn).\n",
    "\n",
    "4. **Select Features:**\n",
    "   - **Step**: Choose a subset of the top-ranked features based on the desired number\n",
    "    or a predetermined threshold for relevance.\n",
    "\n",
    "5. **Train Model:**\n",
    "   - **Step**: Train the predictive model using the selected subset of features and \n",
    "    evaluate its performance on a validation set.\n",
    "\n",
    "By applying filter methods, you can quickly identify and select features that \n",
    "exhibit a strong statistical relationship with the target variable, potentially \n",
    "improving the model's ability to predict customer churn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110bb5a0-b2c0-43cc-807a-7ec05b837167",
   "metadata": {},
   "source": [
    "QTS.7\n",
    "\n",
    "**Embedded Feature Selection for Soccer Match Outcome Prediction:**\n",
    "1. **Choose a Model with Embedded Feature Selection:**\n",
    "   - **Selection**: Opt for a machine learning algorithm that inherently \n",
    "    incorporates feature selection within its \n",
    "    training process. Examples include Lasso Regression, Decision Trees, \n",
    "    Random Forests, Gradient Boosting Machines, or Elastic Net Regression.\n",
    "\n",
    "2. **Train the Model:**\n",
    "   - **Process**: Train the chosen model using the entire dataset, \n",
    "    including all available features.\n",
    "\n",
    "3. **Evaluate Feature Importance:**\n",
    "   - **Process**: Utilize the model's built-in feature importance scores or \n",
    "    coefficients to identify the relevance of each feature in predicting soccer match outcomes.\n",
    "\n",
    "4. **Select Top Features:**\n",
    "   - **Process**: Choose a subset of the most important features based on their scores.\n",
    "    Set a threshold or select a predetermined number of features.\n",
    "\n",
    "5. **Refine and Validate:**\n",
    "   - **Process**: Refine the model by training it again using only the selected \n",
    "    subset of features. Validate the model's performance on a separate test set to ensure generalization.\n",
    "\n",
    "Embedded methods automatically assess feature importance during the model training process,\n",
    "making them suitable for selecting relevant features in a soccer match outcome prediction project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab2de42-b7c0-4845-bf27-6e0223fc3106",
   "metadata": {},
   "source": [
    "QTS.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650e3c92-e1c5-4861-8fa2-2466329c6540",
   "metadata": {},
   "source": [
    "**Wrapper Method for House Price Prediction:**\n",
    "1. **Define Evaluation Metric:**\n",
    "   - **Step**: Choose an evaluation metric (e.g., mean squared error for regression)\n",
    "    to assess the model's performance.\n",
    "\n",
    "2. **Generate Feature Subsets:**\n",
    "   - **Step**: Create different subsets of features (combinations) to be evaluated\n",
    "    by the chosen machine learning model.\n",
    "\n",
    "3. **Train and Evaluate Model:**\n",
    "   - **Step**: Iteratively train the model using each subset of features and evaluate\n",
    "    its performance using the defined metric. This involves training and testing the \n",
    "    model multiple times with different feature combinations.\n",
    "\n",
    "4. **Select Optimal Subset:**\n",
    "   - **Step**: Choose the subset of features that results in the best model performance\n",
    "    according to the evaluation metric.\n",
    "\n",
    "5. **Validate Model:**\n",
    "   - **Step**: Validate the final model's performance on a separate test set to ensure\n",
    "    its generalization ability.\n",
    "\n",
    "Wrapper methods involve the model itself in the feature selection process, assessing \n",
    "subsets of features based on their impact on the model's performance for the specific\n",
    "prediction task, such as predicting house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a5b4a5-cb61-447d-bb12-819baeaffc8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
